#!/usr/bin/env bash

source ./hparams.bash

#params=$(
#	for index in "${!CROSS_TASK_HPARAMS[@]}"
#	do
#		echo "--${CROSS_TASK_HPARAMS[$index]}=${CIFAR_100[$index]}"
#	done
#)
#params+=("--task=cifar100" "--experiment_name=test" "--server_optimizer=yogi")

# python main.py --task=cifar100 \
# --experiment_name=test_2 \
# --server_optimizer=yogi \
# --server_learning_rate=1 \
# --clients_per_round=10 \
# --client_epochs_per_round=1 \
# --client_datasets_random_seed=1 \
# --total_rounds=4000 \
# --client_optimizer=sgd \
# --client_learning_rate=0.03162 \
# --client_batch_size=20 \
# --cifar100_crop_size=24 \
# --server_yogi_epsilon=0.1 \
# --server_yogi_initial_accumulator_value=0.0

#bazel run main:federated_trainer -- --task='shakespeare' \
#--experiment_name='shakepseare_test_1' \
#--server_optimizer=adagrad \
#--server_learning_rate=0.31623 \
#--server_adagrad_epsilon=0.1 \
#--clients_per_round=10 \
#--client_epochs_per_round=1 \
#--client_datasets_random_seed=1 \
#--total_rounds=1200 \
#--client_optimizer=sgd \
#--client_learning_rate=1.0 \
#--client_batch_size=4 \
#--server_adagrad_initial_accumulator_value=0.0 \
#--shakespeare_sequence_length=80

#dp example
bazel run run_federated -- \
  --client_optimizer=sgd \
  --server_optimizer=sgd \
  --server_sgd_momentum=0.9 \
  --clients_per_round=100 \
  --uniform_weighting=True \
  --clip=0.1 \
  --target_unclipped_quantile=0.5 \
  --adaptive_clip_learning_rate=0.2 \
  --noise_multiplier=0.1 \
  --task=stackoverflow_nwp \
  --client_learning_rate=0.3 \
  --server_learning_rate=3 \
  --total_rounds=1500 \
  --client_batch_size=16 \
  --root_output_dir=/tmp/dp \
  --experiment_name=so_nwp

# adaptive lr decay
#bazel run :federated_trainer -- --task=cifar100 --total_rounds=100
#--client_optimizer=sgd --client_learning_rate=0.1 --server_optimizer=sgd
#--server_learning_rate=0.1 --clients_per_round=10 --client_epochs_per_round=1
#--experiment_name=cifar100_classification
